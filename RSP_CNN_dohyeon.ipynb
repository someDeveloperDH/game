{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/someDeveloperDH/game/blob/main/RSP_CNN_dohyeon.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "참고 블로그\n",
        "\n",
        "https://medium.com/@msmapark2/vgg16-%EB%85%BC%EB%AC%B8-%EB%A6%AC%EB%B7%B0-very-deep-convolutional-networks-for-large-scale-image-recognition-6f748235242a\n",
        "\n",
        "https://blog.naver.com/2hannseok/223229158970\n",
        "\n",
        "https://diseny.tistory.com/entry/%ED%98%BC%EB%8F%99%ED%96%89%EB%A0%ACconfusion-matrix?category=906035\n"
      ],
      "metadata": {
        "id": "A60ExDzbg8h1"
      },
      "id": "A60ExDzbg8h1"
    },
    {
      "cell_type": "code",
      "source": [
        "#구글 드라이브 마운트\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRXmYfsvdliV",
        "outputId": "a4cda004-37ee-4b4b-a42e-56428059d2ce"
      },
      "id": "VRXmYfsvdliV",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#필요한거 import\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import os"
      ],
      "metadata": {
        "id": "C9NRKwzldvEg"
      },
      "id": "C9NRKwzldvEg",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = '/content/drive/MyDrive/RPS'  # 필요시 경로 수정\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "test_dir = os.path.join(base_dir, 'test')\n",
        "\n",
        "# 이미지 크기 128x128로 설정 (100x100도 해봤는데 정확도 30퍼나옴..)\n",
        "img_size = (128, 128)\n",
        "\n",
        "# 데이터 증강\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "metadata": {
        "id": "dos-AYhseCS9"
      },
      "id": "dos-AYhseCS9",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#generator 설정(_g)\n",
        "train_g = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=16,  # 배치 크기 16으로 함\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_g = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=img_size,\n",
        "    batch_size=16,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QX_86j9JeLxq",
        "outputId": "da2007ab-dd86-4373-aeef-66f53d297819"
      },
      "id": "QX_86j9JeLxq",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1950 images belonging to 3 classes.\n",
            "Found 270 images belonging to 3 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터셋에 맞게 스텝 조정\n",
        "epochstep = train_g.samples // train_g.batch_size\n",
        "val_step = val_g.samples // val_g.batch_size\n",
        "\n",
        "# VGG16 모델 사용\n",
        "vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(img_size[0], img_size[1], 3))\n",
        "\n",
        "#모든 레이어 freeze\n",
        "for layer in vgg_base.layers:\n",
        "    layer.trainable = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9XoI2B_fGNn",
        "outputId": "78b98075-75d9-40d6-e6c7-520e5a62009e"
      },
      "id": "r9XoI2B_fGNn",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(vgg_base)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer=optimizers.Adam(learning_rate=1e-5),  # 학습률을 줄임\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 조기 종료 설정\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('/content/drive/MyDrive/RPS/model_checkpoint.keras', save_best_only=True, monitor='val_loss')\n",
        "\n",
        "history = model.fit(\n",
        "    train_g,\n",
        "    steps_per_epoch=epochstep,\n",
        "    epochs=10,\n",
        "    validation_data=val_g,\n",
        "    validation_steps=val_step,\n",
        "    callbacks=[early_stopping, checkpoint]\n",
        ")\n",
        "\n",
        "\n",
        "# 혼동 행렬 계산\n",
        "Y_pred = model.predict(val_g, val_g.samples // val_g.batch_size + 1)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "\n",
        "cm = confusion_matrix(val_g.classes, y_pred)\n",
        "print(cm)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0BwYm-EqFlvk",
        "outputId": "be852be9-d531-401b-bf4e-4755725440c0"
      },
      "id": "0BwYm-EqFlvk",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "121/121 [==============================] - 37s 299ms/step - loss: 1.1587 - accuracy: 0.4105 - val_loss: 0.8617 - val_accuracy: 0.6875\n",
            "Epoch 2/10\n",
            "121/121 [==============================] - 22s 176ms/step - loss: 0.9142 - accuracy: 0.5760 - val_loss: 0.7235 - val_accuracy: 0.7891\n",
            "Epoch 3/10\n",
            "121/121 [==============================] - 22s 181ms/step - loss: 0.7848 - accuracy: 0.6587 - val_loss: 0.6299 - val_accuracy: 0.8281\n",
            "Epoch 4/10\n",
            "121/121 [==============================] - 21s 172ms/step - loss: 0.7163 - accuracy: 0.7208 - val_loss: 0.5655 - val_accuracy: 0.8555\n",
            "Epoch 5/10\n",
            "121/121 [==============================] - 22s 180ms/step - loss: 0.6505 - accuracy: 0.7554 - val_loss: 0.5261 - val_accuracy: 0.8555\n",
            "Epoch 6/10\n",
            "121/121 [==============================] - 23s 187ms/step - loss: 0.5901 - accuracy: 0.7813 - val_loss: 0.4647 - val_accuracy: 0.8906\n",
            "Epoch 7/10\n",
            "121/121 [==============================] - 21s 176ms/step - loss: 0.5532 - accuracy: 0.8020 - val_loss: 0.4305 - val_accuracy: 0.9062\n",
            "Epoch 8/10\n",
            "121/121 [==============================] - 20s 166ms/step - loss: 0.5229 - accuracy: 0.8066 - val_loss: 0.3938 - val_accuracy: 0.9062\n",
            "Epoch 9/10\n",
            "121/121 [==============================] - 23s 189ms/step - loss: 0.4929 - accuracy: 0.8170 - val_loss: 0.3793 - val_accuracy: 0.9023\n",
            "Epoch 10/10\n",
            "121/121 [==============================] - 21s 172ms/step - loss: 0.4665 - accuracy: 0.8382 - val_loss: 0.3624 - val_accuracy: 0.8945\n",
            "17/17 [==============================] - 2s 64ms/step\n",
            "[[31 37 22]\n",
            " [27 36 27]\n",
            " [28 31 31]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}